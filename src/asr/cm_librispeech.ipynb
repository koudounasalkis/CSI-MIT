{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cJVqXw3dRSX-"
      },
      "source": [
        "# Configuring the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTOyqWt3RXfU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import librosa\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
        "from jiwer import wer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import divexplorer \n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', None)\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from utils_analysis import filter_itemset_df_by_attributes, slice_by_itemset\n",
        "\n",
        "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
        "from divexplorer.FP_Divergence import FP_Divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTbn0utIRbgS"
      },
      "outputs": [],
      "source": [
        "## Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading pretrained features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load dataset\n",
        "dataset_train = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.360\")\n",
        "dataset_valid = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\")\n",
        "dataset_test = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(dataset_train), len(dataset_valid), len(dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(set(dataset_train['speaker_id'])), len(set(dataset_valid['speaker_id'])), len(set(dataset_test['speaker_id']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load hidden states and logits\n",
        "print(\"Loading train features...\")\n",
        "avg_hidden_states_train = torch.load('pretrained/librispeech/avg_hidden_states_train.pt')\n",
        "last_hidden_states_train = torch.load('pretrained/librispeech/last_hidden_states_train.pt')\n",
        "logits_concatenation_train = torch.load('pretrained/librispeech/logits_concatenation_train.pt')\n",
        "sequence_lengths_train = torch.load('pretrained/librispeech/sequence_lengths_train.pt')\n",
        "transcriptions_train = torch.load('pretrained/librispeech/transcriptions_train.pt')\n",
        "wers_train = torch.load('pretrained/librispeech/wers_train.pt')\n",
        "\n",
        "print(\"Loading valid features...\")\n",
        "avg_hidden_states_valid = torch.load('pretrained/librispeech/avg_hidden_states_valid.pt')\n",
        "last_hidden_states_valid = torch.load('pretrained/librispeech/last_hidden_states_valid.pt')\n",
        "logits_concatenation_valid = torch.load('pretrained/librispeech/logits_concatenation_valid.pt')\n",
        "sequence_lengths_valid = torch.load('pretrained/librispeech/sequence_lengths_valid.pt')\n",
        "transcriptions_valid = torch.load('pretrained/librispeech/transcriptions_valid.pt')\n",
        "wers_valid = torch.load('pretrained/librispeech/wers_valid.pt')\n",
        "\n",
        "print(\"Loading test features...\")\n",
        "avg_hidden_states_test = torch.load('pretrained/librispeech/avg_hidden_states_test.pt')\n",
        "last_hidden_states_test = torch.load('pretrained/librispeech/last_hidden_states_test.pt')\n",
        "logits_concatenation_test = torch.load('pretrained/librispeech/logits_concatenation_test.pt')\n",
        "sequence_lengths_test = torch.load('pretrained/librispeech/sequence_lengths_test.pt')\n",
        "transcriptions_test = torch.load('pretrained/librispeech/transcriptions_test.pt')\n",
        "wers_test = torch.load('pretrained/librispeech/wers_test.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P_SSWxldSmc-"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_train = (np.array(dataset_train[\"text\"]) == np.array(transcriptions_train)).astype(int)\n",
        "prediction_valid = (np.array(dataset_valid[\"text\"]) == np.array(transcriptions_valid)).astype(int)\n",
        "prediction_test = (np.array(dataset_test[\"text\"]) == np.array(transcriptions_test)).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Confidence Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Confidence model\n",
        "class ConfidenceModel(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_size=500, output_size=1):\n",
        "        super(ConfidenceModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.GELU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)\n",
        "                                     \n",
        "    def forward(self,x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.sigmoid(self.linear3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train, valid and test\n",
        "def train(model, inputs, labels, criterion, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def val(model, inputs, labels, criterion):\n",
        "    model.eval()\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    return outputs, loss.item()\n",
        "\n",
        "def test(model, inputs, labels=None, criterion=None):\n",
        "    model.eval()\n",
        "    if labels is None and criterion is None:\n",
        "        outputs = model(inputs.float())\n",
        "        return outputs\n",
        "    else:\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        return outputs, loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = ConfidenceModel(input_size=768, hidden_size=100, output_size=1)\n",
        "model = model.to(device)\n",
        "summary(model, input_size=(768,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = 100\n",
        "BATCH_SIZE = 4096\n",
        "NUM_SUBGROUPS = 2\n",
        "EPOCHS = 10000\n",
        "MIN_SUP = 0.05\n",
        "PRETRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DivExplorer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "speakers = {}\n",
        "with open('data/librispeech/SPEAKERS.TXT', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for i,line in enumerate(lines):\n",
        "            speaker_id = line.strip().split(' ')[0]\n",
        "            if len(speaker_id) == 2:\n",
        "                gender = line.strip().split(' ')[4]\n",
        "            elif len(speaker_id) == 3:\n",
        "                gender = line.strip().split(' ')[3]\n",
        "            else:\n",
        "                gender = line.strip().split(' ')[2]\n",
        "            speakers[speaker_id] = gender\n",
        "\n",
        "gender_train = [speakers[str(sID)] for sID in dataset_train[\"speaker_id\"]]\n",
        "dataset_train = dataset_train.add_column(\"gender\", gender_train)\n",
        "\n",
        "gender_valid = [speakers[str(sID)] for sID in dataset_valid[\"speaker_id\"]]\n",
        "dataset_valid = dataset_valid.add_column(\"gender\", gender_valid) \n",
        "\n",
        "gender_test = [speakers[str(sID)] for sID in dataset_test[\"speaker_id\"]]\n",
        "dataset_test = dataset_test.add_column(\"gender\", gender_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('data/librispeech/speech_metadata_train.csv')\n",
        "df_train['gender'] = gender_train\n",
        "df_train['WER'] = wers_train\n",
        "df_train['id'] = dataset_train['id']\n",
        "\n",
        "df_valid = pd.read_csv('data/librispeech/speech_metadata_valid.csv')\n",
        "df_valid['gender'] = gender_valid\n",
        "df_valid['WER'] = wers_valid\n",
        "df_valid['id'] = dataset_valid['id']\n",
        "\n",
        "df_test = pd.read_csv('data/librispeech/speech_metadata_test.csv')\n",
        "df_test['gender'] = gender_test\n",
        "df_test['WER'] = wers_test\n",
        "df_test['id'] = dataset_test['id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Divide the training set into training and heldout\n",
        "dataset_train_1 = dataset_train.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "## Keep only the rows belonging to the training set\n",
        "df_train_new = df_train[df_train['id'].isin(dataset_train_1['train']['id'])].reset_index(drop=True)\n",
        "df_heldout = df_train[df_train['id'].isin(dataset_train_1['test']['id'])].reset_index(drop=True)\n",
        "\n",
        "len(df_train_new), len(df_heldout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Define abbreviations for plot and visualization\n",
        "from divexplorer.FP_Divergence import abbreviateDict\n",
        "abbreviations = {\n",
        "    'total_silence': 'tot_silence', \\\n",
        "    'speaker_id' : 'spkID', \\\n",
        "    'trimmed': 'trim', \\\n",
        "    'total_':'tot_', \\\n",
        "    'speed_rate_word_trimmed': 'speakRate_trim', \\\n",
        "    'trim_duration': 'trim_dur', \\\n",
        "    'speed_rate_word':'speakRate', \\\n",
        "    'speed_rate_char':'speakCharRate', \\\n",
        "    'duration': 'dur'\n",
        "    }\n",
        "\n",
        "abbreviations_shorter = abbreviations.copy()\n",
        "\n",
        "## Function for sorting data cohorts\n",
        "def sortItemset(x, abbreviations={}):\n",
        "    x = list(x)\n",
        "    x.sort()\n",
        "    x = \", \".join(x)\n",
        "    for k, v in abbreviations.items():\n",
        "        x = x.replace(k, v)\n",
        "    return x\n",
        "\n",
        "def attributes_in_itemset(itemset, attributes, alls = True):\n",
        "    \"\"\" Check if attributes are in the itemset (all or at least one)\n",
        "    \n",
        "    Args:\n",
        "        itemset (frozenset): the itemset\n",
        "        attributes (list): list of itemset of interest\n",
        "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
        "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
        "        \n",
        "    \"\"\"\n",
        "    # Avoid returning the empty itemset (i.e., info of entire dataset)\n",
        "    if itemset == frozenset() and attributes:\n",
        "        return False\n",
        "    \n",
        "    for item in itemset:\n",
        "        # Get the attribute\n",
        "        attr_i = item.split(\"=\")[0]\n",
        "        \n",
        "        #If True, check if ALL attributes of the itemset are the input attributes.\n",
        "        if alls:\n",
        "            # Check if the attribute is present. If not, the itemset is not admitted\n",
        "            if attr_i not in attributes:\n",
        "                return False\n",
        "        else:\n",
        "            # Check if least one attribute. If yes, return True\n",
        "            if attr_i in attributes:\n",
        "                return True\n",
        "    if alls:\n",
        "        # All attributes of the itemset are indeed admitted\n",
        "        return True\n",
        "    else:\n",
        "        # Otherwise, it means that we find None\n",
        "        return False\n",
        "    \n",
        "def filter_itemset_df_by_attributes(df: pd.DataFrame, attributes: list, alls = True, itemset_col_name: str = \"itemsets\") -> pd.DataFrame:\n",
        "    \"\"\"Get the set of itemsets that have the attributes in the input list (all or at least one)\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): the input itemsets (with their info). \n",
        "        attributes (list): list of itemset of interest\n",
        "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
        "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
        "        itemset_col_name (str) : the name of the itemset column, \"itemsets\" as default\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: the set of itemsets (with their info)\n",
        "    \"\"\"\n",
        "\n",
        "    return df.loc[df[itemset_col_name].apply(lambda x: attributes_in_itemset(x, attributes, alls = alls))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Target for DivExplorer: 'WER'\n",
        "target_col = 'WER' \n",
        "target_metric = 'd_outcome'\n",
        "target_div = f'd_{target_col}'\n",
        "t_value_col = 't_value_outcome'\n",
        "printable_columns = ['support', 'itemsets','WER', 'd_WER', 't_value']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Columns for visualization\n",
        "remapped_cols = { \n",
        "       \"outcome\": target_col, \n",
        "       \"d_outcome\": target_div, \n",
        "       t_value_col: 't_value'}\n",
        "show_cols = [\n",
        "       'support', \n",
        "       'itemsets', \n",
        "       target_col, \n",
        "       target_div, \n",
        "       'support_count', \n",
        "       'length', \n",
        "       't_value'\n",
        "       ]\n",
        "\n",
        "## Columns of the df file that we are going to analyze \n",
        "demo_cols = ['gender']\n",
        "\n",
        "signal_cols = ['total_silence', 'total_duration', 'n_words', 'speed_rate_word']\n",
        "\n",
        "input_cols = demo_cols + signal_cols "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discretize the dataframe\n",
        "from divergence_utils import discretize\n",
        "\n",
        "df_discretized = discretize(\n",
        "    df_train_new[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
        "\n",
        "## Create dict of Divergence df\n",
        "fp_diver = FP_DivergenceExplorer(df_discretized, target_name=target_col)\n",
        "FP_fm = fp_diver.getFrequentPatternDivergence(min_support=MIN_SUP, metrics=[target_metric])\n",
        "FP_fm.rename(columns=remapped_cols, inplace=True)\n",
        "FP_fm = FP_fm[show_cols].copy()\n",
        "FP_fm['WER'] = round(FP_fm['WER'], 5)\n",
        "FP_fm['d_WER'] = round(FP_fm['d_WER'], 5)\n",
        "FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
        "fp_divergence = FP_Divergence(FP_fm, target_div)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Compute the divergence for Wav2Vec2-Base\n",
        "FPdiv = fp_divergence.getDivergence(th_redundancy=0.001)[::-1] \n",
        "\n",
        "## Retrieve Most Divergent Itemsets \n",
        "from copy import deepcopy\n",
        "pr = FPdiv.head(NUM_SUBGROUPS).copy()\n",
        "pr[\"support\"] = pr[\"support\"].round(2)\n",
        "pr[\"WER\"] = (pr[\"WER\"]*100).round(3)\n",
        "pr[\"d_WER\"] = (pr[\"d_WER\"]*100).round(3)\n",
        "display(pr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "\n",
        "df_discretized[\"subgID\"] = 0\n",
        "itemsets = []\n",
        "\n",
        "for i in range(NUM_SUBGROUPS):\n",
        "    itemsets.append(list(pr.itemsets.values[i]))\n",
        "\n",
        "for i in tqdm(range(0, len(df_discretized))):\n",
        "    \n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized.loc[i, ks] == vs):\n",
        "            if df_discretized.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized.loc[df_discretized[\"subgID\"]==i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discretize the dataframe\n",
        "from divergence_utils import discretize\n",
        "\n",
        "df_discretized_valid = discretize(\n",
        "    df_valid[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized_valid[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized_valid[signal_cols[i]].replace(replace_values, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "\n",
        "df_discretized_valid[\"subgID\"] = 0\n",
        "for i in tqdm(range(0, len(df_discretized_valid))):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized_valid.loc[i, ks] == vs):\n",
        "            if df_discretized_valid.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized_valid.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized_valid.loc[df_discretized_valid[\"subgID\"]==i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CM Pretraining and Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cm = df_train_new[[\n",
        "    'total_silence', 'n_words', 'speed_rate_word'\n",
        "    ]]\n",
        "df_cm_valid = df_valid[[\n",
        "    'total_silence', 'n_words', 'speed_rate_word'\n",
        "    ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretraining the CM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = torch.cat((\n",
        "    torch.tensor(logits_concatenation_train[:len(df_train_new)]),\n",
        "    torch.tensor(sequence_lengths_train[:len(df_train_new)]).unsqueeze(dim=1),\n",
        "    torch.tensor(last_hidden_states_train[:len(df_train_new)]).squeeze(),\n",
        "    torch.tensor(df_cm['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_cm['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_cm['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_train = torch.tensor(prediction_train[:len(df_train_new)]).unsqueeze(1)\n",
        "\n",
        "X_val = torch.cat((\n",
        "    torch.tensor(logits_concatenation_valid),\n",
        "    torch.tensor(sequence_lengths_valid).unsqueeze(dim=1),\n",
        "    torch.tensor(last_hidden_states_valid).squeeze(),\n",
        "    torch.tensor(df_cm_valid['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_valid['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_cm_valid['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_val = torch.tensor(prediction_valid).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seeds = [1, 10, 42] \n",
        "\n",
        "for seed in seeds:\n",
        "\n",
        "    SEED = seed\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    best_auc = 0\n",
        "    best_acc = 0\n",
        "    best_output = 0\n",
        "    best_model = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    ## Create model\n",
        "    model = ConfidenceModel(\n",
        "        input_size=X_train.shape[1],\n",
        "        hidden_size=HIDDEN_SIZE, \n",
        "        output_size=1\n",
        "        ).to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.NAdam(model.parameters(), lr=0.005) #NAdam\n",
        "\n",
        "    ## Train model\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_aucs = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        \n",
        "        ## Train in batches\n",
        "        for i in range(0, len(X_train), BATCH_SIZE):\n",
        "            train_output, train_loss = train(\n",
        "                model, \n",
        "                X_train[i:i+BATCH_SIZE].float().to(device), \n",
        "                y_train[i:i+BATCH_SIZE].float().to(device), \n",
        "                criterion, \n",
        "                optimizer\n",
        "                )\n",
        "        train_losses.append(train_loss)\n",
        "            \n",
        "        val_output, val_loss = val(\n",
        "            model, \n",
        "            X_val.float().to(device), \n",
        "            y_val.float().to(device),\n",
        "            criterion\n",
        "            )\n",
        "        val_losses.append(val_loss)\n",
        "        val_output = (val_output > 0.5).float()\n",
        "        val_acc = accuracy_score(y_val, val_output.cpu().detach().numpy())\n",
        "        val_auc = roc_auc_score(y_val, val_output.cpu().detach().numpy())\n",
        "        val_aucs.append(val_auc)     \n",
        "     \n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_acc = val_acc\n",
        "            best_output = val_output\n",
        "            best_model = model\n",
        "            best_epoch = epoch\n",
        "\n",
        "        if epoch > 1000:\n",
        "            if val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
        "                break\n",
        "\n",
        "    ## Print metrics \n",
        "    print(\"Best epoch: \", best_epoch)\n",
        "    print(\"Val accuracy: \", round(best_acc*100, 2), \"%\")\n",
        "    print(\"Val AUC: \", round(best_auc, 2))\n",
        "\n",
        "    ## Save model\n",
        "    torch.save(best_model, f'cm_pt_ft/librispeech/confidence_model_pt.pt')\n",
        "    print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenging Subgroups Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create train, val, test split\n",
        "y_train_subs = torch.tensor(df_discretized['subgID'])\n",
        "y_val_subs = torch.tensor(df_discretized_valid['subgID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seeds = [1, 10, 42]\n",
        "\n",
        "for seed in seeds:\n",
        "\n",
        "    SEED = seed\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    best_f1macro = 0\n",
        "    best_acc = 0\n",
        "    best_output = 0\n",
        "    best_epoch = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    if PRETRAIN:\n",
        "        best_model.linear3 = nn.Linear(\n",
        "            HIDDEN_SIZE, \n",
        "            NUM_SUBGROUPS+1\n",
        "            ).to(device)\n",
        "        model = best_model\n",
        "    else:\n",
        "        model = ConfidenceModel(\n",
        "            input_size=X_train.shape[1],\n",
        "            hidden_size=HIDDEN_SIZE, \n",
        "            output_size=NUM_SUBGROUPS+1\n",
        "            ).to(device)\n",
        "\n",
        "    ## Criterion and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
        "\n",
        "    ## Train and validate model\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_output, train_loss = train(\n",
        "            model, \n",
        "            X_train.to(device), \n",
        "            y_train_subs.to(device), \n",
        "            criterion, \n",
        "            optimizer\n",
        "            )\n",
        "        val_output, val_loss = val(\n",
        "            model, \n",
        "            X_val.to(device), \n",
        "            y_val_subs.to(device), \n",
        "            criterion\n",
        "            )\n",
        "        val_output = val_output.cpu().detach().numpy()\n",
        "        val_output = np.argmax(val_output, axis=1)\n",
        "        val_acc = accuracy_score(y_val_subs, val_output)\n",
        "        val_f1 = f1_score(y_val_subs, val_output, average='macro')\n",
        "        if val_f1 > best_f1macro:\n",
        "            best_f1macro = val_f1\n",
        "            best_acc = val_acc\n",
        "            best_output = val_output\n",
        "            best_epoch = epoch\n",
        "    \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if epoch > 5000 and val_loss >= val_losses[-2] and val_loss >= val_losses[-3]:\n",
        "            break\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch: {epoch} | Train Loss: {train_loss} | Val Loss: {val_loss}')\n",
        "\n",
        "    ## Print best accuracy and F1 macro\n",
        "    print(\"Best Epoch: \", best_epoch)\n",
        "    print(\"Best Accuracy: \", best_acc)\n",
        "    print(\"Best F1 Macro: \", best_f1macro)\n",
        "    # print(\"Confusion Matrix: \\n\", confusion_matrix(y_test_subs, best_output))\n",
        "    print(\"--------------------\\n\")\n",
        "\n",
        "    ## Save model\n",
        "    torch.save(model, f'cm_pt_ft/librispeech/confidence_model_ft.pt')\n",
        "    print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Select New Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discretize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discretize the dataframe\n",
        "from divergence_utils import discretize\n",
        "\n",
        "df_discretized_heldout = discretize(\n",
        "    df_heldout[input_cols+[target_col]],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized_heldout[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized_heldout[signal_cols[i]].replace(replace_values, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict Challenging Subgroup IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_heldout = torch.cat((\n",
        "    torch.tensor(logits_concatenation_train[len(df_train_new):]),\n",
        "    torch.tensor(sequence_lengths_train[len(df_train_new):]).unsqueeze(dim=1),\n",
        "    torch.tensor(last_hidden_states_train[len(df_train_new):]).squeeze(),\n",
        "    torch.tensor(df_heldout['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_heldout['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_heldout['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_train_heldout = torch.tensor(prediction_train[len(df_train_new):]).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.load(f'cm_pt_ft/librispeech/confidence_model_ft.pt')\n",
        "\n",
        "train_left_out_output = test(\n",
        "    model,\n",
        "    X_train_heldout.to(device),\n",
        "    )\n",
        "train_left_out_output = train_left_out_output.cpu().detach().numpy()\n",
        "train_left_out_output = np.argmax(train_left_out_output, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve the rows in df_left_out for which train_left_out_output is different from 0\n",
        "df_heldout['subgID'] = train_left_out_output\n",
        "print(len(df_heldout))\n",
        "\n",
        "divergent_samples = df_heldout.loc[df_heldout['subgID']!=0]\n",
        "print(len(divergent_samples))\n",
        "num_samples = len(divergent_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = list(divergent_samples['id'])\n",
        "\n",
        "## Save ids as txt\n",
        "with open('divergent_samples_librispeech_csi.txt', 'w') as f:\n",
        "    for item in ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Random baseline: assing each sample a random sample\n",
        "random_pred = np.random.randint(0, NUM_SUBGROUPS+1, len(X_train_heldout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve the rows in df_left_out for which most_frequent_pred is different from 0\n",
        "df_heldout['subgID'] = random_pred\n",
        "print(len(df_heldout))\n",
        "\n",
        "divergent_samples = df_heldout.loc[df_heldout['subgID']!=0]\n",
        "print(len(divergent_samples))\n",
        "divergent_samples = divergent_samples.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "divergent_samples = divergent_samples[:num_samples]\n",
        "print(len(divergent_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = list(divergent_samples['id'])\n",
        "\n",
        "## Save ids as txt\n",
        "with open('divergent_samples_librispeech_random.txt', 'w') as f:\n",
        "    for item in ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KNN Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## KNN baseline that assigns each sample to the most frequent class among its k nearest neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "SEED = 1\n",
        "best_acc = 0\n",
        "best_f1 = 0\n",
        "best_k = 0\n",
        "\n",
        "for k in range(2,10):\n",
        "    \n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train_subs)\n",
        "\n",
        "    knn_pred = knn.predict(X_val)\n",
        "    acc = accuracy_score(y_val_subs, knn_pred)\n",
        "    f1 = f1_score(y_val_subs, knn_pred, average='macro')\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_k = k\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "\n",
        "print(\"Best K: \", best_k)\n",
        "print(\"Accuracy: \", best_acc)\n",
        "print(\"F1 Macro: \", best_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_heldout = torch.cat((\n",
        "    torch.tensor(logits_concatenation_train[len(df_train_new):]),\n",
        "    torch.tensor(sequence_lengths_train[len(df_train_new):]).unsqueeze(dim=1),\n",
        "    torch.tensor(last_hidden_states_train[len(df_train_new):]).squeeze(),\n",
        "    torch.tensor(df_heldout['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_heldout['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_heldout['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_train_heldout = torch.tensor(prediction_train[len(df_train_new):]).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn.fit(X_train, y_train_subs)\n",
        "\n",
        "knn_pred = knn.predict(X_train_heldout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve the rows in df_left_out for which knn_pred is different from 0\n",
        "df_heldout['subgID'] = knn_pred\n",
        "print(len(df_heldout))\n",
        "\n",
        "divergent_samples = df_heldout.loc[df_heldout['subgID']!=0]\n",
        "print(len(divergent_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = list(divergent_samples['id'])\n",
        "\n",
        "## Save ids as txt\n",
        "with open('divergent_samples_librispeech_knn.txt', 'w') as f:\n",
        "    for item in ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CM Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_heldout = torch.cat((\n",
        "    torch.tensor(logits_concatenation_train[len(df_train_new):]),\n",
        "    torch.tensor(sequence_lengths_train[len(df_train_new):]).unsqueeze(dim=1),\n",
        "    torch.tensor(last_hidden_states_train[len(df_train_new):]).squeeze(),\n",
        "    torch.tensor(df_heldout['total_silence']).unsqueeze(1),\n",
        "    torch.tensor(df_heldout['n_words']).unsqueeze(1),\n",
        "    torch.tensor(df_heldout['speed_rate_word']).unsqueeze(1),\n",
        "    ), dim=1)\n",
        "y_train_heldout = torch.tensor(prediction_train[len(df_train_new):]).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_model = torch.load(f'cm_pt_ft/librispeech/confidence_model_pt.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_left_out_output = test(\n",
        "    cm_model,\n",
        "    X_train_heldout.to(device),\n",
        "    )\n",
        "train_left_out_output = train_left_out_output.cpu().detach().numpy()\n",
        "train_left_out_output = np.argmax(train_left_out_output, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve the rows in df_heldout for which train_left_out_output is different from 0\n",
        "df_heldout['subgID'] = train_left_out_output\n",
        "print(len(df_heldout))\n",
        "\n",
        "divergent_samples = df_heldout.loc[df_heldout['subgID']!=0]\n",
        "print(len(divergent_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = list(divergent_samples['id'])\n",
        "\n",
        "## Save ids as txt\n",
        "with open('divergent_samples_librispeech_cm.txt', 'w') as f:\n",
        "    for item in ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Oracle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "from datasets import load_dataset, Audio\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "def is_target_text_in_range(ref):\n",
        "    if ref.strip() == \"ignore time segment in scoring\":\n",
        "        return False\n",
        "    else:\n",
        "        return ref.strip() != \"\"\n",
        "\n",
        "def get_text(sample):\n",
        "    return sample[\"utt\"]\n",
        "\n",
        "whisper_norm = BasicTextNormalizer()\n",
        "\n",
        "def normalise(batch):\n",
        "    batch[\"norm_text\"] = whisper_norm(get_text(batch))\n",
        "    return batch\n",
        "\n",
        "def data(dataset):\n",
        "    for i, item in enumerate(dataset):\n",
        "        yield {**item[\"audio\"], \"reference\": item[\"norm_text\"]}\n",
        "\n",
        "batch_size = 32\n",
        "whisper_asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
        "\n",
        "whisper_asr.model.config.forced_decoder_ids = (\n",
        "    whisper_asr.tokenizer.get_decoder_prompt_ids(language=args.language, task=\"transcribe\")\n",
        "    )\n",
        "\n",
        "dataset = Dataset.from_pandas(df_heldout)\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "dataset = dataset.map(normalise)\n",
        "dataset = dataset.filter(is_target_text_in_range, input_columns=[\"norm_text\"])\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# run streamed inference\n",
        "for out in tqdm(whisper_asr(data(dataset), batch_size=batch_size)):\n",
        "    predictions.append(whisper_norm(out[\"text\"]))\n",
        "    references.append(out[\"reference\"][0])\n",
        "\n",
        "wer = wer_metric.compute(references=references, predictions=predictions)\n",
        "wer = round(100 * wer, 2)\n",
        "cer = cer_metric.compute(references=references, predictions=predictions)\n",
        "cer = round(100 * cer, 2)\n",
        "\n",
        "print(\"WER:\", wer)\n",
        "print(\"CER:\", cer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Take only the samples for which the prediction is not correct, i.e., the WER is not 0\n",
        "df_heldout['wer'] = wer\n",
        "\n",
        "df_heldout = df_heldout.loc[df_heldout['wer']!=0]\n",
        "print(len(df_heldout))\n",
        "\n",
        "## Save the ids\n",
        "ids = list(df_heldout['id'])\n",
        "with open('divergent_samples_librispeech_supervised_oracle.txt', 'w') as f:\n",
        "    for item in ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_discretized_rest = df_heldout[[f'speech_cluster_id_{k}' for k in [num_clusters]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Number of problematic subgroups: \", NUM_SUBGROUPS)\n",
        "\n",
        "fp_divergence_i = fp_divergence_dict[config]\n",
        "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
        "pr_bot = FPdiv.head(NUM_SUBGROUPS).copy()\n",
        "itemsets = []\n",
        "for i in range(NUM_SUBGROUPS):\n",
        "    itemsets.append(list(pr_bot.itemsets.values[i])[0])\n",
        "\n",
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "df_discretized_rest[\"subgID\"] = 0\n",
        "for i in range(0, len(df_discretized_rest)):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        k, v = itemset.split(\"=\")\n",
        "        if df_discretized_rest.loc[i, k] == int(v):\n",
        "            if df_discretized_rest.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized_rest.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "## Take only the ones different from 0\n",
        "df_discretized_heldout = df_discretized_rest.loc[df_discretized_rest[\"subgID\"]!=0]\n",
        "\n",
        "## Save the ids\n",
        "ids = list(df_discretized_heldout['id'])\n",
        "with open('divergent_samples_librispeech_clustering.txt', 'w') as f:\n",
        "    for item in ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metadata Oracle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discretize the dataframe\n",
        "from divergence_utils import discretize\n",
        "\n",
        "df_discretized_heldout = discretize(\n",
        "    df_heldout[input_cols+[target_col]+['id']],\n",
        "    bins=3,\n",
        "    attributes=input_cols,\n",
        "    strategy=\"quantile\", \n",
        "    round_v = 2,\n",
        "    min_distinct=5,\n",
        ")\n",
        "\n",
        "## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
        "replace_values = {}\n",
        "\n",
        "for i in range(0,len(signal_cols)):\n",
        "\n",
        "    for v in df_discretized_heldout[signal_cols[i]].unique():\n",
        "        if \"<=\" == v[0:2]:\n",
        "            replace_values[v] = \"low\"\n",
        "        elif \">\" == v[0]:\n",
        "            replace_values[v] = \"high\"\n",
        "        elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
        "            replace_values[v] = \"medium\"\n",
        "        else:\n",
        "            raise ValueError(v)\n",
        "\n",
        "    df_discretized_heldout[signal_cols[i]].replace(replace_values, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Create a column in the df, and assign a class to each sample:\n",
        "# - 1 if the sample is in the most divergent itemset\n",
        "# - 2 if the sample is in the second most divergent itemset\n",
        "# - 3 if the sample is in the third most divergent itemset\n",
        "# - ...\n",
        "# - 0 otherwise\n",
        "\n",
        "df_discretized_heldout[\"subgID\"] = 0\n",
        "for i in tqdm(range(0, len(df_discretized_heldout))):\n",
        "    for value,itemset in enumerate(itemsets):\n",
        "        ks = []\n",
        "        vs = []\n",
        "        for item in itemset:\n",
        "            k, v = item.split(\"=\")\n",
        "            ks.append(k)\n",
        "            vs.append(v)\n",
        "        if all(df_discretized_heldout.loc[i, ks] == vs):\n",
        "            if df_discretized_heldout.loc[i, \"subgID\"] == 0:\n",
        "                df_discretized_heldout.loc[i, \"subgID\"] = value+1\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "for i in range(0,NUM_SUBGROUPS+1):\n",
        "    print(len(df_discretized_heldout.loc[df_discretized_heldout[\"subgID\"]==i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Take only the ones different from 0\n",
        "df_discretized_heldout = df_discretized_heldout.loc[df_discretized_heldout[\"subgID\"]!=0]\n",
        "\n",
        "## Save the ids\n",
        "ids = list(df_discretized_heldout['id'])\n",
        "with open('divergent_samples_librispeech_metadata_oracle.txt', 'w') as f:\n",
        "    for item in ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SUPERB - IC Task (FSC).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('amazon': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8dce71f01f4cf7d979b7741b7fb8d94cd1b30c77e0541871108952dcff484f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
